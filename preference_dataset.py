import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import argparse
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Model, load_model
from keras.applications.xception import Xception, preprocess_input
from pickle import dump, load
import random

max_length = 32
tokenizer = load(open("/content/drive/MyDrive/CV Project - Image caption generator/Models/tokenizer.p","rb"))
#model = load_model('/content/drive/MyDrive/CV Project - Image caption generator/Models/model_rl.h5')
model = load_model('/content/drive/MyDrive/CV Project - Image caption generator/Models/model_11.h5')
xception_model = Xception(include_top=False, pooling="avg")

def extract_features(filename, model):
        try:
            image = Image.open(filename)

        except:
            print("ERROR: Couldn't open image! Make sure the image path and extension is correct")
        image = image.resize((299,299))
        image = np.array(image)
        # for images that has 4 channels, we convert them into 3 channels
        if image.shape[2] == 4: 
            image = image[..., :3]
        image = np.expand_dims(image, axis=0)
        image = image/127.5
        image = image - 1.0
        feature = model.predict(image)
        return feature

def word_for_id(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

def sample_pred(preds, temperature=1.0):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    preds = preds.flatten() # flatten the array to make it 1D
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)

def generate_desc(model, tokenizer, photo, max_length):
    in_text = 'start'
    for i in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        preds = model.predict([photo, sequence], verbose=0)
        preds = preds.reshape((preds.shape[1],))
        next_index = sample_pred(preds, temperature=0.5) #adjust the temperature parameter to control the randomness of predictions
        word = word_for_id(next_index, tokenizer)
        if word is None:
            break
        in_text += ' ' + word
        if word == 'end':
            break
    return in_text


strings = os.listdir(dataset_images)
batch_size = 100

# using a list comprehension to create a list of lists, each containing 100 strings
batches = [strings[i:i+batch_size] for i in range(0, len(strings), batch_size)]

# print the first batch as an example
print(len(batches))
xception_model = Xception(include_top=False, pooling="avg")

"""
Prepare a dataset with Three columns

Column 1: Image features from Xception model
Column 2: Caption generated by NN Model
Column 3: Sample Human Feedback
"""

import csv

preference_dataset = []

try:
    #for img in os.listdir(dataset_images): # Number of RL iterations
    for img in batches[4][61:]:
        image_path = dataset_images + "/" + img
        image = Image.open(image_path)
        #image = image. Resize(300,300)
        image.show()
        photo = extract_features(image_path, xception_model)
        in_text1 = 'start'
        prob1 = 1.0
        for i in range(max_length):
            # Generate two captions for the same image
            sequence = tokenizer.texts_to_sequences([in_text1])[0]
            sequence = pad_sequences([sequence], maxlen=max_length)
            preds1 = model.predict([photo, sequence], verbose=0)
            next_index1 = sample_pred(preds1, temperature=0.5)
            word1 = word_for_id(next_index1, tokenizer)
            if word1 is None:
                break
            in_text1 += ' ' + word1
            prob1 *= preds1[0][next_index1]
            if word1 == 'end':
                break

        # Gather feedback from user and store in preference dataset
        print("Caption 1:", in_text1)
        feedback1 = float(input("Rate the quality of caption 1 (0-1): "))
        # Calculate probability of each caption being correct
        prob1 *= feedback1
        preference_dataset.append((np.array(photo), in_text1, prob1))

    columns = ["image", "caption1", "feedback1"]
    with open("preference_dataset.csv", "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(columns)
        
        # Iterate over the preference dataset and write each row to the CSV file
        for data in preference_dataset:
            photo = data[0]
            caption1 = data[1]
            feedback1 = data[2]
            writer.writerow([photo, caption1, feedback1])

    print("**************Finished**************")        
except KeyboardInterrupt:
    print("Interrupted! Saving preference dataset...")
    columns = ["image", "caption1", "feedback1"]
    with open("preference_dataset.csv", "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(columns)
        
        # Iterate over the preference dataset and write each row to the CSV file
        for data in preference_dataset:
            photo = data[0]
            caption1 = data[1]
            feedback1 = data[2]
            writer.writerow([photo, caption1, feedback1])

print("Finished!")
