# -*- coding: utf-8 -*-
"""RL based Image captioning project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YPaCGAvcH76unqIEgJ9K7_kGMUgLZ7Vj
"""

!pip install --upgrade keras

from google.colab import drive
drive.mount('/content/drive')

DEBUG = False

def debug(str):
  if DEBUG:
    print(str)

import string
import numpy as np
from PIL import Image
import os
from pickle import dump, load
import numpy as np

from nltk.translate.bleu_score import sentence_bleu

from keras.applications.xception import Xception, preprocess_input
#from keras.preprocessing.image import load_img, img_to_array
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from tensorflow.keras.layers import add
from keras.models import Model, load_model
from keras.layers import Input, Dense, LSTM, Embedding, Dropout

# small library for seeing the progress of loops.
from tqdm import tqdm_notebook as tqdm
tqdm().pandas()

def load_document(filename):
    file = open(filename, 'r')
    text = file.read()
    file.close()
    return text

def create_image_caption_dictionary(filename):
  captions = load_document(filename)
  splited_captions = captions.split('\n')
  debug(captions.split('\n'))
  descriptions = {}
  for i in splited_captions[1:-1]:
    i = str(i)
    debug(i)
    image, caption = i.split(',',1)
    if image not in descriptions:
      descriptions[image] = [caption]
    else:
      descriptions[image].append(caption)
  return descriptions

descriptions = create_image_caption_dictionary('/content/drive/MyDrive/CV Project - Image caption generator/archive (1)/captions.txt')

def printDictionary(dictionary, prnt = False):
  if prnt:
    keys = list(dictionary.keys())
    print(keys)
    for i in keys:
      print("Image: {}, captions: {}".format(i, descriptions.get(i)))
  pass

printDictionary(descriptions, prnt = False) #set prnt to true to visulize the pre processed text data for the image captions.

def cleaning_text(captions):
    table = str.maketrans('','',string.punctuation)
    for img,caps in captions.items():
        for i,img_caption in enumerate(caps):
            img_caption.replace("-"," ")
            desc = img_caption.split()
            #converts to lowercase
            desc = [word.lower() for word in desc]
            #remove punctuation from each token
            desc = [word.translate(table) for word in desc]
            #remove hanging 's and a 
            desc = [word for word in desc if(len(word)>1)]
            #remove tokens with numbers in them
            desc = [word for word in desc if(word.isalpha())]
            #convert back to string
            img_caption = ' '.join(desc)
            captions[img][i]= img_caption
    return captions

cln_descriptions = cleaning_text(descriptions)
debug(cln_descriptions)

def text_vocabulary(descriptions):
    vocab = set()
    for key in descriptions.keys():
        [vocab.update(d.split()) for d in descriptions[key]]
    return vocab

vocabulary = text_vocabulary(cln_descriptions)
debug(vocabulary)
print("Length of vocabulary = ", len(vocabulary))

#Build reference for finding a BLEU score of the captions generated

def build_reference(descriptions):
    reference = []
    for value in descriptions.values():
        for i in range(len(value)):
          reference.append(value[i].split())
    return reference

reference= build_reference(cln_descriptions)
debug(reference)
print("Length of vocabulary = ", len(reference))
print(reference[:100])

def save_descriptions(descriptions, filename):
    lines = list()
    for key, desc_list in descriptions.items():
        for desc in desc_list:
            lines.append(key + '\t' + desc )
    data = "\n".join(lines)
    file = open(filename,"w")
    file.write(data)
    file.close()

save_descriptions(cln_descriptions, "/content/drive/MyDrive/CV Project - Image caption generator/descriptions.txt")

"""# Feature Extraction using transfer learning - XCEPTION"""

def extract_features(directory):
        model = Xception( include_top=False, pooling='avg' )
        features = {}
        for img in tqdm(os.listdir(directory)):
            filename = directory + "/" + img
            image = Image.open(filename)
            image = image.resize((299,299))
            image = np.expand_dims(image, axis=0)
            #image = preprocess_input(image)
            image = image/127.5
            image = image - 1.0
            feature = model.predict(image)
            features[img] = feature
        return features

dataset_images = "/content/drive/MyDrive/CV Project - Image caption generator/archive (1)/Images"
#2048 feature vector
#features = extract_features(dataset_images)
#dump(features, open("features.p","wb"))

features = load(open("/content/drive/MyDrive/CV Project - Image caption generator/features.p","rb"))

def load_photos(filename):
    file = load_document(filename)
    photos = file.split("\n")[:-1]
    return photos

def load_clean_descriptions(filename, photos): 
    #loading clean_descriptions
    file = load_document(filename)
    descriptions = {}
    for line in file.split("\n"):
        words = line.split()
        if len(words)<1 :
            continue
        image, image_caption = words[0], words[1:]
        if image in photos:
            if image not in descriptions:
                descriptions[image] = []
            desc = '<start> ' + " ".join(image_caption) + ' <end>'
            descriptions[image].append(desc)
    return descriptions

def load_features(photos):
    #loading all features
    all_features = load(open("/content/drive/MyDrive/CV Project - Image caption generator/features.p","rb"))
    #selecting only needed features
    features = {k:all_features[k] for k in photos}
    return features

filename = "/content/drive/MyDrive/CV Project - Image caption generator/Flickr_8k.trainImages.txt"
#train = loading_data(filename)
train_imgs = load_photos(filename)
train_descriptions = load_clean_descriptions("/content/drive/MyDrive/CV Project - Image caption generator/descriptions.txt", train_imgs)
train_features = load_features(train_imgs)

print("The list of training images are: {}".format(train_imgs))
print("The dictionary of training images with captions: {}".format(train_descriptions))

#converting dictionary to clean list of descriptions
def dict_to_list(descriptions):
    all_desc = []
    for key in descriptions.keys():
        [all_desc.append(d) for d in descriptions[key]]
    return all_desc

from keras.preprocessing.text import Tokenizer
def create_tokenizer(descriptions):
    desc_list = dict_to_list(descriptions)
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(desc_list)
    return tokenizer

tokenizer = create_tokenizer(train_descriptions)
dump(tokenizer, open('tokenizer.p', 'wb'))
vocab_size = len(tokenizer.word_index) + 1
print("Vocabulary size of the tokens: {}".format(vocab_size))

#calculate maximum length of descriptions
def max_length(descriptions):
    desc_list = dict_to_list(descriptions)
    return max(len(d.split()) for d in desc_list)
    
max_length = max_length(descriptions)
print("Maximum length of the descrption to find the parameter for RNN: {}".format(max_length))

def data_generator(descriptions, features, tokenizer, max_length):
    while 1:
        for key, description_list in descriptions.items():
            #retrieve photo features
            feature = features[key][0]
            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)
            yield [[input_image, input_sequence], output_word]

def create_sequences(tokenizer, max_length, desc_list, feature):
    X1, X2, y = list(), list(), list()
    # walk through each description for the image
    for desc in desc_list:
        # encode the sequence
        seq = tokenizer.texts_to_sequences([desc])[0]
        # split one sequence into multiple X,y pairs
        for i in range(1, len(seq)):
            # split into input and output pair
            in_seq, out_seq = seq[:i], seq[i]
            # pad input sequence
            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
            # encode output sequence
            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
            # store
            X1.append(feature)
            X2.append(in_seq)
            y.append(out_seq)
    return np.array(X1), np.array(X2), np.array(y)

#You can check the shape of the input and output for your model
[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))
a.shape, b.shape, c.shape
#((47, 2048), (47, 32), (47, 7577))

"""
from keras.utils import plot_model

# define the captioning model
def define_model(vocab_size, max_length):

    # features from the CNN model squeezed from 2048 to 256 nodes
    inputs1 = Input(shape=(2048,))
    fe1 = Dropout(0.3)(inputs1)
    fe2 = Dense(256, activation='relu')(fe1)
    fe2 = Dense(512, activation='relu')(fe1)

    # LSTM sequence model
    inputs2 = Input(shape=(max_length,))
    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
    se2 = Dropout(0.3)(se1)
    se3 = LSTM(256)(se2)
    se3 = LSTM(512)(se2)

    # Merging both models
    decoder1 = add([fe2, se3])
    decoder2 = Dense(256, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)

    # tie it together [image, seq] [word]
    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    model.compile(loss='categorical_crossentropy', optimizer='adam')

    # summarize model
    print(model.summary())
    plot_model(model, to_file='model.png', show_shapes=True)

    return model

"""

from keras.utils import plot_model

# define the captioning model
def define_model(vocab_size, max_length):

    # features from the CNN model squeezed from 2048 to 256 nodes
    inputs1 = Input(shape=(2048,))
    fe1 = Dropout(0.3)(inputs1)
    fe2 = Dense(256, activation='relu', name = 'image_layer_1')(fe1)
    fe2 = Dense(512, activation='relu', name = 'image_layer_2')(fe2)

    # LSTM sequence model
    inputs2 = Input(shape=(max_length,), name='input_sequence')
    se1 = Embedding(vocab_size, 256, mask_zero=True, name='embedding_1')(inputs2)
    se2 = Dropout(0.3)(se1)
    se3 = LSTM(256, activation = 'relu', name = 'LSTM_layer_1')(se2)
    se4 = Dense(512, activation = 'relu', name = 'LSTM_layer_2')(se3)

    # Merging both models
    decoder1 = add([fe2, se4])
    decoder2 = Dense(512, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)

    # tie it together [image, seq] [word]
    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    model.compile(loss='categorical_crossentropy', optimizer='adam')

    # summarize model
    print(model.summary())
    plot_model(model, to_file='model.png', show_shapes=True)

    return model

# train our model
print('Dataset: ', len(train_imgs))
print('Descriptions: train=', len(train_descriptions))
print('Photos: train=', len(train_features))
print('Vocabulary Size:', vocab_size)
print('Description Length: ', max_length)

model = define_model(vocab_size, max_length)
epochs = 10
steps = len(train_descriptions)
# making a directory models to save our models
#os.mkdir("models")
for i in range(epochs):
    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)
    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)
    model.save("models/model_" + str(i) + ".h5")

"""# Testing the model"""

import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import argparse
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Model, load_model
from keras.applications.xception import Xception, preprocess_input
from pickle import dump, load
import random

"""
ap = argparse.ArgumentParser()
ap.add_argument('-i', '--image', required=True, help="Image Path")
args = vars(ap.parse_args())
img_path = args['image']
"""

def extract_features(filename, model):
        try:
            image = Image.open(filename)

        except:
            print("ERROR: Couldn't open image! Make sure the image path and extension is correct")
        image = image.resize((299,299))
        image = np.array(image)
        # for images that has 4 channels, we convert them into 3 channels
        if image.shape[2] == 4: 
            image = image[..., :3]
        image = np.expand_dims(image, axis=0)
        image = image/127.5
        image = image - 1.0
        feature = model.predict(image)
        return feature

def word_for_id(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

def sample_pred(preds, temperature=1.0):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    preds = preds.flatten() # flatten the array to make it 1D
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)

def generate_desc(model, tokenizer, photo, max_length):
    in_text = 'start'
    for i in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        preds = model.predict([photo, sequence], verbose=0)
        preds = preds.reshape((preds.shape[1],))
        next_index = sample_pred(preds, temperature=0.5) #adjust the temperature parameter to control the randomness of predictions
        word = word_for_id(next_index, tokenizer)
        if word is None:
            break
        in_text += ' ' + word
        if word == 'end':
            break
    return in_text

#path = 'Flicker8k_Dataset/111537222_07e56d5a30.jpg'
max_length = 32
tokenizer = load(open("/content/drive/MyDrive/CV Project - Image caption generator/Models/tokenizer.p","rb"))
#model = load_model('/content/drive/MyDrive/CV Project - Image caption generator/Models/model_rl.h5')
model = load_model('/content/drive/MyDrive/CV Project - Image caption generator/Models/model_11.h5')
xception_model = Xception(include_top=False, pooling="avg")

img_path = "/content/test_image4.png"
photo = extract_features(img_path, xception_model)
img = Image.open(img_path)

description = generate_desc(model, tokenizer, photo, max_length)
print("\n\n")
print("Caption 1: ", description)

plt.imshow(img)

print('BLEU score -> {}'.format(sentence_bleu(reference, description.split() )))

"""# Approach to RLHF
Initialize the RL model with the pre-trained LSTM model, Xception model, and tokenizer.


Generate a textual description of the image using the generate_desc() function.


Display the generated description to the user and ask for feedback in the range of 0 to 1 for the quality of the caption. For example, you could ask the user to rate the caption on a scale of 0 to 1, with 0 indicating a completely inaccurate or nonsensical caption, and 1 indicating a perfectly accurate and informative caption.


Use the user feedback to update the model's parameters. One way to do this is by treating the feedback as a reward signal and using a reinforcement learning algorithm, such as Q-learning or policy gradient methods, to optimize the model's parameters to maximize the expected reward.


Repeat steps 2-4 for a number of iterations, gradually refining the model's parameters based on the user feedback.



"""

import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import argparse
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Model, load_model
from keras.applications.xception import Xception, preprocess_input
from pickle import dump, load

max_length = 32
dataset_images = "/content/drive/MyDrive/CV Project - Image caption generator/archive (1)/Images"
tokenizer = load(open("/content/drive/MyDrive/CV Project - Image caption generator/Models/tokenizer.p","rb"))
model = load_model('/content/drive/MyDrive/CV Project - Image caption generator/Models/model_11.h5')
xception_model = Xception(include_top=False, pooling="avg")

# Initialize RL model parameters
alpha = 0.1 # Learning rate
epsilon = 0.1 # Exploration rate
gamma = 0.9 # Discount factor
q_table = np.random.rand(max_length, len(tokenizer.word_index) + 1)
RL_learning_loop = 0

# Update model parameters using RL algorithm
for img in os.listdir(dataset_images): # Number of RL iterations
    RL_learning_loop += 1
    if RL_learning_loop < 10:
      image_path = dataset_images + "/" + img
      image = Image.open(image_path)
      #image = image.resize(300,300)
      image.show()
      photo = extract_features(image_path, xception_model)
      in_text = 'start'
      for i in range(max_length):
          sequence = tokenizer.texts_to_sequences([in_text])[0]
          sequence = pad_sequences([sequence], maxlen=max_length)
          preds = model.predict([photo,sequence], verbose=0)
          next_index = sample_pred(preds, temperature=0.5) # introduce randomness with temperature=0.5
          next_word = word_for_id(next_index, tokenizer)
          if next_word is None:
              break
          in_text += ' ' + next_word
          if next_word == 'end':
              break

      print("Generated caption:", in_text)
      feedback = float(input("Rate the quality of the caption (0-1): "))
      for i in range(len(in_text.split())):
          word = in_text.split()[i]
          action = tokenizer.word_index[word]
          if i == len(in_text.split())-1:
              next_max_q_value = 0
          else:
              next_word = in_text.split()[i+1]
              next_action = tokenizer.word_index[next_word]
              next_max_q_value = np.max(q_table[i+1][next_action])
          q_table[i][action] += alpha * (feedback + gamma * next_max_q_value - q_table[i][action])


      # Update model weights based on Q-table
      for layer in model.layers:
        if layer.name == 'embedding_1':
          weights = layer.get_weights()[0]
          for word, index in tokenizer.word_index.items():
            if index > len(q_table):
              break
            for timestep in range(max_length):
              action = timestep, index
              if action in q_table:
                q_value = q_table[action]
                weights[index] += alpha * q_value / np.max(q_table)
          layer.set_weights([weights])
        elif layer.name == 'LSTM_layer_1':
          weights = layer.get_weights()
          for timestep in range(max_length):
            action = timestep, timestep
            if action in q_table:
              q_value = q_table[action]
              weights[0][timestep,:] += alpha * q_value / np.max(q_table)
          layer.set_weights(weights)

            # Save updated model and Q-table
      model.save('models/model_rl.h5')
      dump(q_table, open("q_table.p", "wb"))

#path = 'Flicker8k_Dataset/111537222_07e56d5a30.jpg'
max_length = 32
tokenizer = load(open("/content/drive/MyDrive/CV Project - Image caption generator/Models/tokenizer.p","rb"))
#model = load_model('/content/drive/MyDrive/CV Project - Image caption generator/Models/model_rl.h5')
model = load_model('/content/models/model_rl.h5')
xception_model = Xception(include_top=False, pooling="avg")

img_path = "/content/test_image4.png"
photo = extract_features(img_path, xception_model)
img = Image.open(img_path)


description = generate_desc(model, tokenizer, photo, max_length)
print("\n\n")
print(description)
plt.imshow(img)

print('BLEU score -> {}'.format(sentence_bleu(reference, description.split() )))

# Update model weights based on Q-table
model1 = load_model('/content/drive/MyDrive/CV Project - Image caption generator/Models/model_11.h5')
model2 = load_model("/content/models/model_rl.h5")

for layer in model1.layers:
  #if layer.name == 'embedding_1':
  if layer.name == 'LSTM_layer_1':
    weights1 = layer.get_weights()


for layer in model2.layers:
  #if layer.name == 'embedding_1':
  if layer.name == 'LSTM_layer_1':  
    weights2 = layer.get_weights()
 
# compare the matrices for equality
if np.array_equal(weights1, weights2):
    print("The Weights are equal")
else:
    print("The weights are not equal")

strings = os.listdir(dataset_images)
batch_size = 100

# using a list comprehension to create a list of lists, each containing 100 strings
batches = [strings[i:i+batch_size] for i in range(0, len(strings), batch_size)]

# print the first batch as an example
print(len(batches))
xception_model = Xception(include_top=False, pooling="avg")

"""
Prepare a dataset with Three columns

Column 1: Image features from Xception model
Column 2: Caption generated by NN Model
Column 3: Sample Human Feedback
"""

import csv

preference_dataset = []

try:
    #for img in os.listdir(dataset_images): # Number of RL iterations
    for img in batches[4][61:]:
        image_path = dataset_images + "/" + img
        image = Image.open(image_path)
        #image = image. Resize(300,300)
        image.show()
        photo = extract_features(image_path, xception_model)
        in_text1 = 'start'
        prob1 = 1.0
        for i in range(max_length):
            # Generate two captions for the same image
            sequence = tokenizer.texts_to_sequences([in_text1])[0]
            sequence = pad_sequences([sequence], maxlen=max_length)
            preds1 = model.predict([photo, sequence], verbose=0)
            next_index1 = sample_pred(preds1, temperature=0.5)
            word1 = word_for_id(next_index1, tokenizer)
            if word1 is None:
                break
            in_text1 += ' ' + word1
            prob1 *= preds1[0][next_index1]
            if word1 == 'end':
                break

        # Gather feedback from user and store in preference dataset
        print("Caption 1:", in_text1)
        feedback1 = float(input("Rate the quality of caption 1 (0-1): "))
        # Calculate probability of each caption being correct
        prob1 *= feedback1
        preference_dataset.append((np.array(photo), in_text1, prob1))

    columns = ["image", "caption1", "feedback1"]
    with open("preference_dataset.csv", "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(columns)
        
        # Iterate over the preference dataset and write each row to the CSV file
        for data in preference_dataset:
            photo = data[0]
            caption1 = data[1]
            feedback1 = data[2]
            writer.writerow([photo, caption1, feedback1])

    print("**************Finished**************")        
except KeyboardInterrupt:
    print("Interrupted! Saving preference dataset...")
    columns = ["image", "caption1", "feedback1"]
    with open("preference_dataset.csv", "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(columns)
        
        # Iterate over the preference dataset and write each row to the CSV file
        for data in preference_dataset:
            photo = data[0]
            caption1 = data[1]
            feedback1 = data[2]
            writer.writerow([photo, caption1, feedback1])

print("Finished!")

