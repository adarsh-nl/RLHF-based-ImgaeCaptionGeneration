# -*- coding: utf-8 -*-
"""Feedback Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LoN5ea6f2rrEJLLGV8vb34ZmwzEJjxFj
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Embedding, LSTM, Input, Concatenate
from sklearn.model_selection import train_test_split
import pickle

# Load the data
data = pd.read_csv('/content/main_preference_dataset.csv')

# Define the maximum caption length and vocabulary size
max_len = max([len(caption.split()) for caption in data['caption1']])
max_words = 35

# Tokenize the captions
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(data['caption1'])
captions_tokenized = tokenizer.texts_to_sequences(data['caption1'])
captions_padded = pad_sequences(captions_tokenized, padding='post', maxlen=max_len)

# Convert the image features to arrays
image_features = np.array([np.fromstring(x[1:-1], sep=',') for x in data['image']])

# Split the data into train, val, and test sets
train_captions, test_captions, train_images, test_images, train_labels, test_labels = train_test_split(
    captions_padded, image_features, data['feedback1'], test_size=0.2, random_state=42)

train_captions, val_captions, train_images, val_images, train_labels, val_labels = train_test_split(
    train_captions, train_images, train_labels, test_size=0.2, random_state=42)

# Define the model architecture
caption_input = Input(shape=(max_len,))
caption_embedded = Embedding(max_words, 100)(caption_input)
caption_lstm = LSTM(100, dropout=0.5)(caption_embedded)
image_input = Input(shape=(image_features.shape[1],))
combined = Concatenate()([caption_lstm, image_input])
dense1 = Dense(100, activation='relu')(combined)
dropout1 = Dropout(0.5)(dense1)
output = Dense(1, activation='sigmoid')(dropout1)
model = Model(inputs=[caption_input, image_input], outputs=output)

# Compile the model
model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit([train_captions, train_images], train_labels, epochs=10, batch_size=32,
          validation_data=([val_captions, val_images], val_labels))

# Save the model and tokenizer
model.save('critic_model.h5')
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

# Evaluate the model on the test set
loss, accuracy = model.evaluate([test_captions, test_images], test_labels)
print('Test loss:', loss)
print('Test accuracy:', accuracy)

import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model
import random
import pickle

def preprocess_text(text):
    # Add any additional preprocessing steps if needed
    return text

def predict_rating(model, tokenizer, image_feature, caption):
    # Preprocess the caption
    processed_caption = preprocess_text(caption)

    # Process the input data
    caption_tokenized = tokenizer.texts_to_sequences([processed_caption])
    max_sequence_length = model.input[0].shape[1]  # Use index 0 for caption input
    padded_sequence = pad_sequences(caption_tokenized, maxlen=max_sequence_length)

    # Predict the rating
    prediction = model.predict([padded_sequence, np.array([image_feature])])[0][0]

    # Return the predicted rating
    return prediction


# Load the dataset
data = pd.read_csv('/content/main_preference_dataset.csv')

# Load the trained model and tokenizer
model = load_model('critic_model.h5')
with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

# Select a random sample
random_index = random.randint(0, len(data) - 1)
image_feature = np.fromstring(data['image'][random_index][1:-1], sep=',')
caption = data['caption1'][random_index]

# Predict the rating for the random sample
rating = predict_rating(model, tokenizer, image_feature, caption)

print(f"Random Sample - Predicted Rating: {rating}")

